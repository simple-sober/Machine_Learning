{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wheat Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sknn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-53ab94c46069>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sknn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as err\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sknn.mlp import Regressor, Layer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and concatenating the 2013 and 2014 data to form 1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2013 = pd.read_csv(\"wheat-2013-supervised.csv\")\n",
    "data_2014 = pd.read_csv(\"wheat-2014-supervised.csv\")\n",
    "total_data = pd.concat([data_2013, data_2014],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the datashape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_2013.shape)\n",
    "print(total_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Some Extrapolatory Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2013.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2013.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2014.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are removing the location based data because the yield for a particular location is always the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Hypothesis 1 **: \n",
    "A particular location (Longitude, Latitude) always have the same yield throughout a particular winter wheat yield, the Yield is different for different winters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First rounding the latitude and longitude\n",
    "data_2013[['Latitude','Longitude']] = data_2013[['Latitude','Longitude']].round(6)\n",
    "data_2014[['Latitude','Longitude']] = data_2014[['Latitude','Longitude']].round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe for unique combinations of latitude and longitude\n",
    "lon_lat13 = data_2013.drop_duplicates(['Latitude','Longitude']).ix[:,2:4]\n",
    "lon_lat13 = lon_lat13.reset_index(inplace=False)\n",
    "lon_lat13.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_list = []\n",
    "num_diff_yield = []\n",
    "for i in range(0, len(lon_lat13)):\n",
    "        yield_list.append(data_2013[(data_2013.Latitude == lon_lat13.Latitude[i]) &\n",
    "                                    (data_2013.Longitude == lon_lat13.Longitude[i])].Yield.unique())\n",
    "        num_diff_yield.append(len(data_2013[(data_2013.Latitude == lon_lat13.Latitude[i]) &\n",
    "                                            (data_2013.Longitude == lon_lat13.Longitude[i])].Yield.unique()))\n",
    "\n",
    "df1 = pd.DataFrame({'Location' : list(zip(lon_lat13.Latitude, lon_lat13.Longitude)), \n",
    "                   'Unique_Yields' : yield_list, 'Number_yield': num_diff_yield}, \n",
    "                  columns= ['Location','Unique_Yields', 'Number_yield'])\n",
    "\n",
    "print(\"Number of coordinates with more than 1 yield value: %.4f \" % len(df1[df1.Number_yield > 1]))\n",
    "print(\"Total number of coordinates: %.4f\" %len(df1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis 2:\n",
    "Mostly particular county always produce the same yield throughout a particular winter year but the value of Yield is different for different years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating unique county list\n",
    "countynames= data_2013.CountyName.unique()\n",
    "# creating unique yields for each county\n",
    "Yield_list = []\n",
    "Num_diff_yields = []\n",
    "for county in countynames:\n",
    "    Yield_list.append(data_2013[data_2013.CountyName == county].Yield.unique())\n",
    "    Num_diff_yields.append(len(data_2013[data_2013.CountyName == county].Yield.unique()))\n",
    "df = pd.DataFrame({'County' : countynames, 'Unique_Yields' : Yield_list, 'Number_yield': Num_diff_yields}, \n",
    "                  columns= ['County','Unique_Yields', 'Number_yield'])\n",
    "\n",
    "print(\"Number of counties with more than 1 yield value: %.4f \" % len(df[df.Number_yield > 1]))\n",
    "print(\"Total number of counties: %.4f\" %len(df))\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing location based fields in the data i.e. CountyName, State, Latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_data1 = total_data.drop(['CountyName','State','Latitude','Longitude','Date'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the rows with 'NA' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_data2 = total_data1.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows before and after removing 'na' values\n",
    "print(total_data1.shape[0])\n",
    "print(total_data2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the Feature space and target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(total_data2.columns)\n",
    "cols.remove('Yield')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Train and Test split from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_split, test_split = train_test_split(total_data2, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice #1:\n",
    "We start with a Tree based model rather than a linear model as when the relationship between a feature and the output is conditional upon the values of other features. A tree-based model would be able to capture such a conditionality, but linear models simply cannot.\n",
    "\n",
    "Also, Tree-based models in principle can approximate functions with any \"shape\", whereas linear models can only produce functions with a linear \"shape\" with respect to a chosen set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: RandomForest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "rf_model = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice #2:\n",
    "'n_estimators': The number of trees in the forest\n",
    "                As the number of trees increases the complexity, time taken to run the model increases but the errors                 decreases. But with higher number of trees the model can overfit. Therefore the number of trees should                 be optimal i.e to maintain balance between reduced error and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "rf_model.fit(X=train_split.ix[:,cols],y=train_split.ix[:,'Yield'])\n",
    "\n",
    "# important features\n",
    "imp = list(zip(cols,rf_model.feature_importances_))\n",
    "imp=sorted(imp,key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the feature importance\n",
    "print(imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred1 = rf_model.predict(test_split.ix[:,cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation(Metrics Used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error1 = err(test_split.ix[:,\"Yield\"], pred1)\n",
    "print( \"MSE: %.4f\" % error1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice #3:\n",
    "** Metric used: Mean Square error: **\n",
    "\n",
    "Among various metrics like 'mean absolute error', median absolute error', R2 score and 'Mean squared error'. We choose\n",
    "'mean Square Error'. It refers to the mean of the squared deviation of predicted value from the true valued. It is always positive and a value closer to zero is better.\n",
    "\n",
    "Since the errors are square before they are averaged therefore, the MSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice # 4:\n",
    "The next model, we opted for is Gradient Boosting, as it concentrates on reducing the error rather than fitting trees on random samples of the data and it more robust to overfitting than radom forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'n_estimators' :600, 'learning_rate' : 0.4, 'loss' : 'ls',\n",
    "         'max_depth' : 8}\n",
    "# instantiating the model\n",
    "XGboost_model = GradientBoostingRegressor(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice # 5:\n",
    "The parameters values used in the above model:\n",
    "1. 'n_estimators': The number of boosting stages to perform on the data\n",
    "2. 'learning_rate': learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
    "3. 'loss': 'ls' refers to least squares regression. It is a natural choice for regression due to its superior computational properties.\n",
    "4. 'max_depth': The maximum depth limits the number of nodes in the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "XGboost_model.fit(train_split.ix[:,cols], train_split.ix[:,\"Yield\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions using the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = XGboost_model.predict(test_split.ix[:,cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error2 = err(test_split.ix[:,'Yield'], pred2)\n",
    "print( \"MSE: %.4f\" % error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important features \n",
    "imp2 = list(zip(cols, XGboost_model.feature_importances_))\n",
    "imp2 = sorted(imp2,key=lambda x:x[1])\n",
    "print(imp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice #6:\n",
    "Do get a better idea of the number of stages to boost the tree, we plot the deviance plot on the train and the testing data to get a better sense of the error at each boosting stage.\n",
    "Hypothesis: The plot(Deviance vs Boosting iterations) should follow a exponential curve, with deviance curving to a constant value with increasing number of iterations, so the point where the curve tends to be a straight line we can choose the iteration value as 'n_estimators'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the test and train get deviance vs the boosting iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(XGboost_model.staged_predict(test_split.ix[:,cols])):\n",
    "    test_score[i] = XGboost_model.loss_(test_split.ix[:,\"Yield\"], y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, XGboost_model.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: Feed Forward Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "nn_model = Regressor(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=100),\n",
    "        Layer(\"Linear\")],\n",
    "    learning_rate=0.005,\n",
    "    n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice #7:\n",
    "1. The activation function for the hidden layer is 'Rectifier' with 100 neurons and 'linear' for the output layer\n",
    "2. The learning rate: the learning rate of backpropogation method.\n",
    "3. n_iter: The number of epoch's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice #8:\n",
    "1. It is best to first normalize or standardize the data before inputting it in the neural network, as standardizing the inputs can make training faster and reduce the chances of getting stuck in local optima.\n",
    "2. We need to first to remove the categorical variables (before normalizing) and then add them back after normalizing the rest of the numeric variables, as normalizing the categorical fields will make them lose their basic purpose.\n",
    "3. Also, it is always better to convert the data and response variable into a numpy array before feeding into a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response variable\n",
    "train_response = train_split.ix[:,\"Yield\"]\n",
    "test_response = test_split.ix[:,\"Yield\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing the categorical variable\n",
    "train_split_cat = train_split.ix[:, 9:12]\n",
    "test_split_cat = test_split.ix[:,9:12]\n",
    "\n",
    "# dropping the categorical variable from rest of the data\n",
    "train_split2 = train_split.drop(train_split.columns[[9,10,11,20]],axis=1)\n",
    "test_split2 = test_split.drop(train_split.columns[[9,10,11,20]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the input data\n",
    "train_split_norm = (train_split2 - train_split2.mean())/(train_split2.max() - train_split2.min())\n",
    "test_split_norm = (test_split2 - test_split2.mean())/(test_split2.max() - test_split2.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the response variable\n",
    "train_response_norm = (train_response - train_response.mean())/(train_response.max() - train_response.min())\n",
    "test_response_norm = (test_response - test_response.mean())/(test_response.max() - test_response.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the categorical variable back to the normalized data for training\n",
    "train_split_norm1 = pd.concat([train_split_norm, train_split_cat], axis=1)\n",
    "test_split_norm1 = pd.concat([test_split_norm, test_split_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting the dataframe into a numpy array before feeding in the neural net\n",
    "Xtrain_array = train_split_norm1.ix[:,cols].as_matrix()\n",
    "ytrain_array = train_response_norm.as_matrix()\n",
    "Xtest_array = test_split_norm1.ix[:,cols].as_matrix()\n",
    "ytest_array = test_response.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "nn_model.fit(Xtrain_array, ytrain_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on the test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3= nn_model.predict(Xtest_array)\n",
    "# denormalizing the data\n",
    "pred3_ = (pred3*(test_response.max() - test_response.min())) + test_response.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice #9:\n",
    " I have denormalized the response variable before calculating the MSE value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error3 = err(ytest_array, pred3_)\n",
    "print( \"MSE: %.4f\" % error3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4: K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "knn_model = KNeighborsRegressor(n_neighbors= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Choice #10:\n",
    " I have used the nearest neighbors as 3(default = 5), This is chosen after performing iterations with various different values of the n_neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "knn_model.fit(train_split.ix[:,cols], train_split.ix[:,\"Yield\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on the Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred4 = knn_model.predict(test_split.ix[:,cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error4 = err(test_split.ix[:,\"Yield\"], pred4)\n",
    "print(\"MSE : %.4f\" % error4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Feature Importance\n",
    "#feature_importance = XGboost_model.feature_importances_\n",
    "# make importances relative to max importance\n",
    "#feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "#sorted_idx = np.argsort(feature_importance)\n",
    "#pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "#plt.subplot(1, 2, 2)\n",
    "#plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "#plt.yticks(pos, cols[sorted_idx])  # <-- use something else than 'cols'\n",
    "#plt.xlabel('Relative Importance')\n",
    "#plt.title('Variable Importance')\n",
    "#plt.show()\n",
    "\n",
    "# incomplete"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
